{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e44c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_geometric.loader.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.multiprocessing\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "from GNNDataset import GNNDataset\n",
    "from ClusterDataset import ClusterDataset\n",
    "from ClusterDatasetBuilder import ClusterDatasetBuilder\n",
    "from train_transformer import *\n",
    "from data_statistics import *\n",
    "from GNN_TrackLinkingNet import EarlyStopping, weight_init\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from Transformer import Transformer\n",
    "from lang import Lang\n",
    "from LossFunction import Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce07a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0, number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# CUDA Setup\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}, number of devices: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779bf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 60\n",
    "max_seq_length = 60\n",
    "batch_size = 64\n",
    "converter = Lang(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfda33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05055d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator MaxAbsScaler from version 1.5.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "model_folder = \"/eos/user/c/czeh/\"\n",
    "hist_folder = \"/eos/user/c/czeh/histo_10pion0PU/\"\n",
    "data_folder_training = \"/eos/user/c/czeh/graph_data/processed\"\n",
    "store_folder_training = \"/eos/user/c/czeh/graph_data_trans\"\n",
    "data_folder_test = \"/eos/user/c/czeh/graph_data_test/processed\"\n",
    "store_folder_test = \"/eos/user/c/czeh/graph_data_trans_test\"\n",
    "\n",
    "scaler = joblib.load(\"/eos/user/c/czeh/graph_data/scaler.joblib\")\n",
    "scale = torch.tensor(scaler.scale_).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3343d9b-61fb-4da1-bd4f-08f0ee0fefec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      " 11%|â–ˆ         | 43/400 [04:40<27:06,  4.56s/it] "
     ]
    }
   ],
   "source": [
    "testBuilder = ClusterDatasetBuilder(store_folder_test, data_folder_test, input_length=input_length)\n",
    "\n",
    "if not testBuilder.metadataExists():\n",
    "    testBuilder.generate(24, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29162559-bb40-437b-b55d-e498101de35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBuilder = ClusterDatasetBuilder(store_folder_training, data_folder_training, input_length=input_length)\n",
    "\n",
    "if not trainBuilder.metadataExists():\n",
    "    trainBuilder.generate(24, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ee2c1-6342-44d7-b86e-b431d8dedb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training = ClusterDataset(store_folder_training, input_length=input_length, scale=scale, output_group=False)\n",
    "dataset_test = ClusterDataset(store_folder_test, input_length=input_length, scale=scale, output_group=False, num_workers=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3aff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "num_heads = 2\n",
    "num_layers = 3\n",
    "d_model = 128\n",
    "d_ff = 256\n",
    "dropout = 0.2\n",
    "padding = converter.word2index[\"<PAD>\"]\n",
    "feature_num = len(dataset_test.model_feature_keys)\n",
    "max_nodes = max(dataset_test.max_nodes, dataset_training.max_nodes)\n",
    "vocab_size = max_nodes + 4\n",
    "\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, feature_num, max_nodes, max_seq_length, dropout)\n",
    "weight_init(model)\n",
    "criterion = Loss(converter, vocab_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f99b9-319f-448a-a459-66805675cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.parallel.DistributedDataParallel(model)\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset_training, shuffle=True, batch_size=batch_size)\n",
    "test_dl = DataLoader(dataset_test, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5476f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally introduce weight decay\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Drop Step Size over time\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "early_stopping = EarlyStopping(patience=5, delta=-0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Weights if needed\n",
    "# weights = torch.load(\"/eos/user/c/czeh/tranformer_2.pt\", weights_only=True)\n",
    "# model.load_state_dict(weights[\"model_state_dict\"])\n",
    "# optimizer.load_state_dict(weights[\"optimizer_state_dict\"])\n",
    "# start_epoch = weights[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist = []\n",
    "val_loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ee1a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn\n",
    "# Optionally introduce gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n",
    "\n",
    "fig_loss, ax_loss = plt.subplots(1, 1)\n",
    "fig_loss.set_figwidth(6)\n",
    "fig_loss.set_figheight(3)\n",
    "\n",
    "display_loss = display(1, display_id=True)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 101):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    \n",
    "    loss = train(model, optimizer, test_dl, epoch, criterion, vocab_size, device=device)\n",
    "    print(f\"Training loss: {loss}\")\n",
    "    train_loss_hist.append(loss)\n",
    "    \n",
    "    val_loss = test(model, test_dl, epoch, criterion, vocab_size, device=device)\n",
    "    val_loss_hist.append(val_loss)\n",
    "    print(f\"Validation loss: {val_loss}\")\n",
    "    \n",
    "    ax_loss.clear()\n",
    "    plot_loss(train_loss_hist, val_loss_hist, ax=ax_loss, n=0)\n",
    "    display_loss.update(fig_loss)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}, LR: {scheduler.get_last_lr()[0]}\")\n",
    "    \n",
    "    early_stopping(model, val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping after {epoch+1} epochs with best score {early_stopping.best_score}\")\n",
    "        early_stopping.load_best_model(model)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(10)\n",
    "epochs = len(train_loss_hist)\n",
    "ax.plot(range(1, epochs+1), moving_average(train_loss_hist, 8), label='train', linewidth=2)\n",
    "ax.plot(range(1, epochs+1), moving_average(val_loss_hist, 8), label='val', linewidth=2)\n",
    "ax.set_ylabel(\"Loss\", fontsize=14)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=14)\n",
    "ax.set_title(\"Training and Validation Loss\", fontsize=14)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ba369",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = f\"{datetime.now():%Y-%m-%d-%H}:00\"\n",
    "save_model(model, epoch, optimizer, train_loss_hist, val_loss_hist, model_folder, f\"tranformer_date_{date}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d246b",
   "metadata": {},
   "source": [
    "## Test Full Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EventGrouping import EventGrouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d461c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, feature_num, max_nodes, max_seq_length, dropout).to(device)\n",
    "weights = torch.load(osp.join(model_folder, \"tranformer_date_2025-06-02-16:00.pt\"), weights_only=True)\n",
    "model.load_state_dict(weights[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c404b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = dataset_training.get(0)\n",
    "print(len(components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = EventGrouping(model, seq_length=input_length)\n",
    "\n",
    "nTrackster = 0\n",
    "for component in components:\n",
    "    max_comp_t = int(torch.max(component[\"x\"]).item())\n",
    "    if max_comp_t > nTrackster:\n",
    "        nTrackster = max_comp_t\n",
    "  \n",
    "group = 0\n",
    "edges = np.full(nTrackster, -1)\n",
    "\n",
    "    \n",
    "for component in components:\n",
    "    converter = Lang(trackster_list=component[\"lang\"])\n",
    "    print(\"Goal\", component[\"seq\"], converter.word2index[component[\"root\"]])\n",
    "    res = runner(component)[-1]\n",
    "    print(runner(component)[-1])\n",
    "    \n",
    "    new_groups = converter.seq2y(res.cpu().numpy(), nodes=nTrackster, start_group=group)\n",
    "    print(np.array(range(new_groups.shape[0]))[new_groups >= 0])\n",
    "    edges = np.maximum(edges, converter.seq2y(res.cpu().numpy(), nodes=nTrackster, start_group=group))\n",
    "    print(np.max(edges))\n",
    "    group = np.max(edges) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7eaffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[edges>=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661d336",
   "metadata": {},
   "source": [
    "## Random Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f794e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, feature_num, max_nodes, max_seq_length, dropout).to(device)\n",
    "weights = torch.load(\"/eos/user/c/czeh/tranformer_date_2025-06-02-16:00.pt\", weights_only=True)\n",
    "model.load_state_dict(weights[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa42063",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = dataset_training.get(0)\n",
    "components[0][\"lang\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09220474",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_training.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = components[0][\"nTrackster\"]\n",
    "converter = Lang(trackster_list=components[0][\"lang\"])\n",
    "sample_seq = converter.starting_seq(components[0][\"root\"], input_length).to(device)\n",
    "print(sample_seq)\n",
    "\n",
    "X = components[0][\"x\"].float()\n",
    "X /= scale\n",
    "X = F.pad(X, pad=(0, 0, max_nodes - num_nodes, 0), value=converter.word2index[\"<PAD>\"])\n",
    "X = X[:, list(map(dataset_test.node_feature_dict.get, dataset_test.model_feature_keys))]\n",
    "\n",
    "predictions = model(torch.unsqueeze(X, dim=0), torch.unsqueeze(sample_seq, dim=0))\n",
    "predicted_index = torch.argsort(-predictions[0, -1, :num_nodes], dim=0)\n",
    "print(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b73661",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[targets[:, -1] != -4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173362d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[mask].shape[0]/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774dde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dataset_training.__getitem__(0)[1]\n",
    "opts = torch.roll(opts, -1, dims=0)\n",
    "opts[-1] = 5\n",
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ccda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mask = opts != -4\n",
    "opts[out_mask].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c35fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.reshape(targets[mask], (int(targets[mask].shape[0]/3), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd836c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf31c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
